<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Machine-learning by moazenzadeh-ehsan</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Machine-learning</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/moazenzadeh-ehsan/Machine-Learning" class="btn">View on GitHub</a>
      <a href="https://github.com/moazenzadeh-ehsan/Machine-Learning/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/moazenzadeh-ehsan/Machine-Learning/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h3>
<a id="welcome-to-coursera-prediction-assignment-page" class="anchor" href="#welcome-to-coursera-prediction-assignment-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Welcome to Coursera Prediction Assignment Page.</h3>

<p>by: Ehsan Moazen Zadeh</p>

<p>1_I uploaded packages</p>

<blockquote>
<p>library(caret)</p>
</blockquote>

<p>2_I set an overall seed to make reproducible results</p>

<blockquote>
<p>set.seed(1221)</p>
</blockquote>

<p>3_I read data to R for training and testing</p>

<blockquote>
<p>training&lt;-read.csv("pml-training.csv")</p>

<p>testing&lt;-read.csv("pml-testing.csv")</p>
</blockquote>

<p>3_Exploratory data analysis:</p>

<p>_First, I wanted to have an overall view of the data</p>

<blockquote>
<p>dim(training)</p>

<p>colnames(training)</p>

<p>summary(training)</p>
</blockquote>

<p>_ “summary” code did not help that much as there were 160 variables</p>

<p>_I skipped plotting the data for the same reason of many variables</p>

<p>_I wanted to have an overview of the missing data</p>

<blockquote>
<p>complete.cases(training)</p>
</blockquote>

<p>_much missing data!
So, I considered using “knnImpute” for pre-Processing in later steps</p>

<p>_I wanted to have an overview of the correlations between variables</p>

<blockquote>
<p>M&lt;-abs(cor(training[,-160]))</p>
</blockquote>

<p>_above code did not work because it only works on numeric variables
So, as I couldn’t have an estimate of the correlations, I considered “pca”
as a probable candidate for pre-Processing</p>

<p>_next, I checked for the variables with near zero variance</p>

<blockquote>
<p>nsv&lt;-nearZeroVar(training,saveMetrics=TRUE)</p>

<p>nsv</p>
</blockquote>

<p>_60 near zero variables!!!</p>

<p>4_Covariate creation:</p>

<p>_I tried many ways of coding to exclude the near zero variables from the training set, including “for” loop, “apply”, “rep”. but none of them worked perfectly as I am naïve in coding. Finally I came up with this code:</p>

<blockquote>
<p>for (i in 1:160){</p>
</blockquote>

<pre><code>x&lt;-nsv[i,4]

if (x=="TRUE"){

    training&lt;-training[,-i]

    nsv&lt;-nsv[-i,]

}
</code></pre>

<p>}</p>

<p>_each time that I ran the code above, it reduced the number of variables with near zero variance but produced an error in the midway and did not exclude them completely; however, it would omit all the 60 near zero variables after 4 times of repeating the code! So, I wrote the code 4 times consecutively for now.( I may find a better way to omit the 60 near zeroes in future.) At the end, I checked for the results:</p>

<blockquote>
<p>dim(training)</p>

<p>dim(nsv)</p>
</blockquote>

<p>_my options for pre-Processing were “knnImpute”, ”pca”, “center”, “scale” up to this step. I think all of them were needed if I would choose a regression model rather than a non-parametric tree</p>

<p>5_Final model:</p>

<p>_according to the high number of variables, missing values and the fact that there were 19622 observations, I found the randomForest a proper choice. It would be more accurate than other models for such a data set.</p>

<blockquote>
<p>library(randomForest)</p>
</blockquote>

<p>_as I chose to use randomForest(“rf”), there were no need to apply cross validation methods like “cv” or “boot” through <em>trainControl</em> argument. Here is a link for more information on this matter:
<a href="https://www.stat.berkeley.edu/%7Ebreiman/RandomForests/cc_home.htm">https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></p>

<p>_also, there was no real need for that much pre-Processing in comparison to regression models. So, I just chose “knnImpute” to cover missing data. I used <em>train</em> function once with the “knnImpute” and the other time without “knnImpute”. Because there was not a significant difference in the accuracy of the randomForest model on training data set, I preferred not to use “knnImpute” in order to decrease the amount of computations and time consumption.</p>

<blockquote>
<p>modFit&lt;-train(training$classe~.,data=training,method="rf",prox=TRUE)</p>

<p>modFit</p>

<p>modFit$finalModel</p>
</blockquote>

<p>_Accuracy was 98.92%. Out-of-Bag (oob) error estimate was 0.74%.
 I predicted that there might be an error rate higher than oob when applying the model to the testing set. Also, the really high accuracy of the model would bring the concern of over-fitting. Anyhow, I had no more time to try other models and I decided to submit this one.</p>

<p>_here is the final code for trying the model on the testing data set:</p>

<blockquote>
<p>confusionMatrix(testing$classe,predict(modFit,testing))</p>
</blockquote>

<p>Thank you for reading</p>

<p>(The codes are provided below in sequence)</p>

<pre><code>library(caret)
set.seed(1221)
training&lt;-read.csv("pml-training.csv")
testing&lt;-read.csv("pml-testing.csv")
dim(training)
colnames(training)
summary(training)
complete.cases(training)
M&lt;-abs(cor(training[,-160]))
nsv&lt;-nearZeroVar(training,saveMetrics=TRUE)
nsv
for (i in 1:160){
x&lt;-nsv[i,4]
if (x=="TRUE"){
training&lt;-training[,-i]
nsv&lt;-nsv[-i,]
}
}
for (i in 1:160){
x&lt;-nsv[i,4]
if (x=="TRUE"){
training&lt;-training[,-i]
nsv&lt;-nsv[-i,]
}
}
for (i in 1:160){
x&lt;-nsv[i,4]
if (x=="TRUE"){
training&lt;-training[,-i]
nsv&lt;-nsv[-i,]
}
}

for (i in 1:160){
x&lt;-nsv[i,4]
if (x=="TRUE"){
training&lt;-training[,-i]
nsv&lt;-nsv[-i,]
}
}
dim(training)
dim(nsv)
library(randomForest)
modFit&lt;-train(training$classe~.,data=training,method="rf",prox=TRUE)
modFit
modFit$finalModel
confusionMatrix(testing$classe,predict(modFit,testing))

</code></pre>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Authors and Contributors</h3>

<p><a href="mailto:emplus@Ymail.com">emplus@Ymail.com</a></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/moazenzadeh-ehsan/Machine-Learning">Machine-learning</a> is maintained by <a href="https://github.com/moazenzadeh-ehsan">moazenzadeh-ehsan</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>

